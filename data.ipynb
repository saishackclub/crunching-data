{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-2Kqu9WHhu"
      },
      "source": [
        "# Data Exploration and Scikit-Learn\n",
        "\n",
        "In this final intro notebook, we'll use everything we've learned so far and work with some real datasets. We'll also finally use the Scikit-Learn library (called `sklearn` in Python) to do some basic machine learning on some sample data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDFIigZ5VXSs"
      },
      "source": [
        "## Some important imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_4G-kOwVq4K"
      },
      "source": [
        "In Jupyter Notebooks, we can import the libraries we need once, and then use the libraries on all future code blocks without needing to re-import everything.\n",
        "\n",
        "Run the following code block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KnWCWO8VawZ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sklearn\n",
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o918trVrjUH"
      },
      "source": [
        "## Get that dataset!\n",
        "\n",
        "Before we can start to do machine learning, we need to have data to train our systems on. If you remember in the \"Introduction to Machine Learning\" lesson, datasets aren't always easy to come by, and many are privately collected and owned. Very often, if you want to work on a new machine learning application, you may have to figure out how to collect your own data.\n",
        "\n",
        "However, there are many great services that have open and free datasets that you can use, like [Kaggle](https://www.kaggle.com/). For this lesson, we'll use a dataset from Kaggle that contains Spotify information on the top songs of 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyKab1mtSSG"
      },
      "source": [
        "## Grabbing datasets from GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X6-63sysdM7"
      },
      "source": [
        "We've actually stored the dataset in a different website, called [GitHub](https://github.com/), so that we can easily grab the data for this lesson.\n",
        "\n",
        "Be sure to run each of the code blocks below. They're not written in Python -- they're written in a different language called Bash, which is typically used on the command line and can manipulate things like files on your computer.\n",
        "\n",
        "You don't have to worry about what each of these code blocks does, just that they help us pull data from GitHub so that we have access to it within this Notebook. (Almost like how we need to import libraries before we can start to use them.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhSPs8MUUyIp"
      },
      "outputs": [],
      "source": [
        "# The following is not a Python command, but a bash command\n",
        "# It looks at the files and folders in this current directory\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kMznGGHUzpJ"
      },
      "outputs": [],
      "source": [
        "# This checks if the directory called \"random-data\" exists, and if so, remove it\n",
        "! if [ -d \"random-data\" ]; then rm -r random-data; fi\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj4x7BDiU1GW"
      },
      "outputs": [],
      "source": [
        "# This creates a new directory based on a repo from my GitHub\n",
        "! git clone https://github.com/Devking/random-data.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmnLi3frU3rQ"
      },
      "outputs": [],
      "source": [
        "# This is for us to check that the directory was successfully downloaded\n",
        "! ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbLoBYMVPFG"
      },
      "source": [
        "# Spotify's Top Songs of 2017\n",
        "\n",
        "Let's say that you were interested in music and wanted to create a machine learning system to suggest a new song for someone to listen to. How would we create this?\n",
        "\n",
        "Well first, we would need to have some data on what music people like. Once we have that data, we might start to think about the different qualities of a song, so you can recommend new songs based on similar qualities from songs a person has listened to.\n",
        "\n",
        "The Spotify Top Songs of 2017 dataset contains exactly that information, so let's see how we would start to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iJbHGkbue1H"
      },
      "source": [
        "## Working with Pandas\n",
        "\n",
        "Before we jump into machine learning, let's practice looking at datasets and exploring them. We'll use a library called `pandas` to help us do this.\n",
        "\n",
        "We've already imported `pandas` earlier in the Notebook, so we can use it in the following code blocks. If you did not run the first code block of this lesson, the following code will raise an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAgze1dMuqsm"
      },
      "outputs": [],
      "source": [
        "# This is the file name of the Spotify dataset\n",
        "# It's a \"CSV\" file, which stands for \"comma-separated values file\"\n",
        "# Really, it's just a long text file with a lot of information separated by commas\n",
        "filename = \"random-data/spotify.csv\"\n",
        "\n",
        "# We will use a function built into pandas to read our csv data\n",
        "data = pandas.read_csv(filename)\n",
        "\n",
        "# Finally, we'll print the data to take a look at it\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u56pvsyqvyaa"
      },
      "source": [
        "The print-out of our data above looks... not too informative. This is mainly because there's a lot of data in our dataset that we're working with, and Jupyter Notebooks doesn't format it nicely for us when we directly print all of the data. Instead, we'll have to use some functions in `pandas` to explore the data ourselves.\n",
        "\n",
        "By the way, if you want to see the data in this dataset as a table, you can [click here to see it displayed in GitHub](https://github.com/Devking/random-data/blob/master/spotify.csv). We'll also write some code so that we can visualize our data in table form.\n",
        "\n",
        "There are 2017 rows in this dataset and 17 columns, where each row is a single song and (almost) each column is a **feature** of the song. In terms of dataset size, this is closer to the small side -- some datasets may have thousands or even millions of rows.\n",
        "\n",
        "## Exploring some features\n",
        "\n",
        "Recall that our data in datasets is described by a set of properties called features.\n",
        "\n",
        "For song data, some features you might expect are tempo, time signature, and name.\n",
        "\n",
        "Let's print all of the names of the songs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d05N6hixvUO"
      },
      "outputs": [],
      "source": [
        "# Notice that we didn't just say \"name\"\n",
        "# Like with accessing values in dictionaries, the specific key value in this data set is \"song_title\"\n",
        "print(data[\"song_title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZRM0CDEyL7k"
      },
      "source": [
        "The above code prints the song titles for all 2017 songs in our dataset. However, if we wanted to recommend a new song to someone, it probably wouldn't be enough to just recommend them based on song title. We'll want to make use of multiple features, and it may help us to visualize them to find patterns in the data.\n",
        "\n",
        "How do we know what all of the features in our dataset are? One way is to look directly at the CSV file, because these are usually the column headers that are on the first line of the file. (We could also look at the column headings in GitHub since it displays all of the data in a table nicely for us.) Since we're trying to work with our Python code as much as possible, let's see how we would do it in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqNtL7KdzMC-"
      },
      "outputs": [],
      "source": [
        "filename = \"random-data/spotify.csv\"\n",
        "raw_data = pandas.read_csv(filename)\n",
        "\n",
        "# Get the values of the columns (the features) and print them out\n",
        "features = raw_data.columns.values\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-QOvE2Y0IUz"
      },
      "source": [
        "We see from the printout different features of our songs, such as tempo, acoustiness (how many acoustic sounds, like an acoustic guitar, are in a song), title, and artist. The first column is actually used to index the songs (so we can refer to them by number), and that's why the column title shows as \"Unnamed: 0\". This particular column is not a feature of our data.\n",
        "\n",
        "For now, let's work with just a small amount of our dataset -- four features (tempo, acousticness, song title, and artist) and 20 songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u47Xjj3a0ybR"
      },
      "outputs": [],
      "source": [
        "filename = \"random-data/spotify.csv\"\n",
        "raw_data = pandas.read_csv(filename)\n",
        "\n",
        "# In pandas, we put our data into data frames so that we can manipulate them\n",
        "# This takes two parameters: raw_data specifies that we're using the music data from above\n",
        "# columns specifies what columns of that data we want to keep\n",
        "data = pandas.DataFrame(raw_data, columns=['tempo', 'acousticness', 'song_title', 'artist'])\n",
        "\n",
        "# Remember slicing? We use that to take just the first 20 songs in our dataset\n",
        "data = data[:20]\n",
        "\n",
        "# This will display our data in a nice table\n",
        "# Typically we need to use print(), but our variable 'data' is a pandas DataFrame which has special display properties\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il4aHlcC1mTu"
      },
      "source": [
        "There! Now we have something that's much more manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOezFvkT1_1-"
      },
      "source": [
        "## Comparing features of songs\n",
        "\n",
        "Say we know someone likes the song \"Mask Off\" by Future, and we want to recommend them another song based off of similarity to \"Mask Off\".\n",
        "\n",
        "Perhaps we think that the tempo (or speed) of the song might be helpful, so we'll look for songs that are similar in tempo to \"Mask Off\" (which is tempo = 150.062 beats per minute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDZP76DN2340"
      },
      "outputs": [],
      "source": [
        "# Let's keep track of the Mask Off tempo\n",
        "mask_off_tempo = 150.062\n",
        "\n",
        "# Let's create a new column in our dataset\n",
        "# The value of this column is the difference in tempo from that song and the tempo of \"Mask Off\"\n",
        "# The function abs() takes the absolute value, because we just need to know the difference in tempo,\n",
        "# but not if that tempo is faster or slower than \"Mask Off\"\n",
        "data['tempo_difference'] = abs(mask_off_tempo - data.tempo)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-geY3CTI3a_5"
      },
      "source": [
        "### Visualizing the tempo difference\n",
        "\n",
        "We can look at the column `tempo_difference` in our table above to see which songs have similar tempos to \"Mask Off\". Another way to present this information is to visualize it in a graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwiW0WmF3ygY"
      },
      "outputs": [],
      "source": [
        "# Remember that we've imported matplotlib.pyplot as plt at the start of the Notebook\n",
        "# Here we will use pyplot to do our visualization\n",
        "\n",
        "plt.scatter(data['tempo_difference'], data['song_title'])\n",
        "plt.xlabel('Difference in Tempo')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPgl8VY54FYU"
      },
      "source": [
        "Take a moment to look at the graph and see what's going on. The y-axis of the graph is _discrete_ -- each y-value refers to a specific song. The x-axis of the graph is the tempo difference.\n",
        "\n",
        "Points that are closer to the left have smaller differences in tempo, whereas points that are closer to the right have larger differences in tempo.\n",
        "\n",
        "Looking at the graph above, which song has the largest difference in tempo from \"Mask Off\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mmikhHd5Tw3"
      },
      "source": [
        "### Finding the acousticness difference\n",
        "\n",
        "Let's do the same for acousticness, which we might think is another good measure of song similarity.\n",
        "\n",
        "The acousticness of a song is measured from 0 to 1, with 0 being no acousticness (all electronic sounds) and 1 being all acoustic (imagine an acoustic guitar on its own). The acoustiness of \"Mask Off\" is 0.01020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC_SdXc5u-a"
      },
      "outputs": [],
      "source": [
        "# Add new column for acoustic difference\n",
        "data['acoustic_difference'] = abs(0.01020 - data.acousticness)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQgqXKFi53fX"
      },
      "outputs": [],
      "source": [
        "# Visualize it\n",
        "\n",
        "plt.scatter(data['acoustic_difference'], data['song_title'])\n",
        "plt.xlabel('Difference in Acousticness')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8cHgol58W5"
      },
      "source": [
        "What song is most different in acousticness to \"Mask Off\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcLxATUa6EMZ"
      },
      "source": [
        "## Recommendation based on similarity\n",
        "\n",
        "Now we want to recommend a song based on its similarity to \"Mask Off\", in terms of both tempo and acousticness. Looking at the two graphs above, which song might you say is closest to \"Mask Off\" for both features?\n",
        "\n",
        "Take a moment to look at both graphs and try to decide!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX82zjdt6qg0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "One possible recommendation is the song \"Gyöngyhajú lány\", which is similar in both tempo and acousticness. Let's listen to a bit of the two songs to see if we think that's accurate:\n",
        "\n",
        "> Future - Mask Off (Clean): https://www.youtube.com/watch?v=5G2SjYMy7gk\n",
        "\n",
        "> Omega - Gyöngyhajú lány: https://www.youtube.com/watch?v=CGt-rTDkMcM&feature=youtu.be\n",
        "\n",
        "Though they sound very different from one another, you can indeed say that the features like tempo are similar. However, upon listening to the songs themselves, you might come to the conclusion that they don't _really_ sound similar. Perhaps other features of songs, like genre, may be better features to recommend off of. It's an open question!\n",
        "\n",
        "Part of the work when developing a recommendation system like this is to see which features give better recommendations, and to select what the best features might be. We might know that \"Mask Off\" and \"Gyöngyhajú lány\" are similar songs in terms of tempo and acoustiness, but this might not mean that they're similar with regards to other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YVVTKv29ONf"
      },
      "source": [
        "### Quantifying similarity with multiple features\n",
        "\n",
        "Before we move on to using Scikit-Learn, let's try to quantify the similarity between songs using both tempo and acousticness together. Previously, we had calculated the tempo difference and the acousticness difference separately, and can compare songs based on one feature or the other.\n",
        "\n",
        "When we tried to find the most similar song using both features, we mainly approached it by visually inspecting the two graphs.\n",
        "\n",
        "But how can we calculate similarity with respect to both features, in a quantifiable way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW0fn38Q-OWQ"
      },
      "source": [
        "First, let's see if we can visualize our 20 songs on the same graph, with both tempo and acoustiness features represented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJQRYCUP-WDt"
      },
      "outputs": [],
      "source": [
        "# The x-axis is tempo\n",
        "# The y-axis is acousticness\n",
        "plt.scatter(data['tempo_difference'], data['acoustic_difference'])\n",
        "plt.xlabel('Tempo Difference')\n",
        "plt.ylabel('Acoustic Difference')\n",
        "\n",
        "# Set the figure size of the plot so it's not too small to read\n",
        "plt.rcParams[\"figure.figsize\"] = 10,10\n",
        "\n",
        "# This loop goes through each point on the graph and labels it with the song title\n",
        "for index, row in data.iterrows():\n",
        "    plt.annotate(row['song_title'], xy=(row['tempo_difference'], row['acoustic_difference']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP7Tr32l_b6r"
      },
      "source": [
        "From this visualization, where we look at both features at once, it's a lot clearer that \"Gyöngyhajú lány\" is very similar to \"Mask Off\" in terms of both acousticness and tempo difference, whereas \"Master of None\" is very different.\n",
        "\n",
        "Interesting aside: While \"Mask Off\" was released in the year 2017, \"Master of None\" was released in 2007, but happened to regain popularity in 2017. This is likely because of its use in the Netflix show, \"Master of None\". While a data scientist may look at data in terms of the numbers, a social scientist (like a sociologist or cultural anthropologist) can give context to the data that we're looking at. When creating applications for the real world, we need this sort of context, and it may help us understand how to make better software like recommendation systems. For example, a music expert might be able to provide context that similarity in genre is a better feature to use for recommending a song, rather than similarity in tempo. Working across disciplines and bringing in perspectives from the arts and humanities helps us create better computer programs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM56goXVBMRv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "On the visualization above, we might realize that we can quantify 'similarity' as the distance between two points. The shorter the distance from \"Mask Off\", the more similar the song represented by that point is. We can make use of this intuition to numerically compute similarity across both features.\n",
        "\n",
        "How do we compute distance again? We use the distance formula!\n",
        "\n",
        "$$ distance = \\sqrt{(y_2 - y_1)^{2} + (x_2 - x_1)^{2}} $$\n",
        "\n",
        "In words, we take the squared difference of both features, add them, and then take the square root.\n",
        "\n",
        "Let's say $ (x_2, y_2) $ is our point representing \"Mask Off\", which is at $(0,0)$. To find the distance to the point representing \"Gyöngyhajú lány\", we substitute for $x_1$ and $y_1$ the values of \"Gyöngyhajú lány\", which are 5.908 and 0.01000.\n",
        "\n",
        "Written out:\n",
        "\n",
        "$$ distance = \\sqrt{(0.01 - 0)^{2} + (5.908 - 0)^{2}} $$\n",
        "$$ distance = 5.908 $$\n",
        "\n",
        "We can write this out in code and create a new column based on \"distance\" from \"Mask Off\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JHNiIfvDFly"
      },
      "outputs": [],
      "source": [
        "mask_off_tempo = 150.062\n",
        "mask_off_acousticness = 0.0102\n",
        "data[\"distance\"] = np.sqrt((data.acousticness - mask_off_acousticness) ** 2 + (data.tempo - mask_off_tempo) ** 2)\n",
        "\n",
        "# Let's sort the dataset in terms of distance\n",
        "sorted_dataset_distance = data.sort_values('distance')\n",
        "\n",
        "sorted_dataset_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbXTMsfHElZk"
      },
      "source": [
        "Now we have a column that tells us numerically which song is closest to \"Mask Off\" in terms of both tempo and acousticness!\n",
        "\n",
        "### Normalizing our data\n",
        "\n",
        "But wait a minute... if we take a close look at the `tempo_difference` and `distance` columns, we will notice that the two are very similar. This makes sense, because while tempo values range anywhere from 0 to 161 in our dataset, acousticness only goes from 0 to 1. The `tempo_difference` completely overshadows the `acoustic_difference` when we calculate `distance`.\n",
        "\n",
        "In data science and machine learning, it's very common for different features to work on completely different scales. This means that we cannot compare two features directly against one another, without first **normalizing** the data to the same scale across all features. Typically, normalizing our data means scaling the values of our features to a range between 0 and 1.\n",
        "\n",
        "We won't cover normalizing in this notebook, but it's good to be aware that this is a typical process that we perform on our data before using it to train a machine learning system."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
